{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytesseract for OCR\n",
    "\n",
    "- This practical covers the use of python library **pytesseract** to perform OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pytesseract as tess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "**Reading text from a Single Image file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a lot of 12 point text to test the\n",
      "ocr code and see if it works on all types\n",
      "of file format.\n",
      "\n",
      "The quick brown dog jumped over the\n",
      "lazy fox. The quick brown dog jumped\n",
      "over the lazy fox. The quick brown dog\n",
      "jumped over the lazy fox. The quick\n",
      "brown dog jumped over the lazy fox.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading an image\n",
    "img = Image.open('test.png')\n",
    "\n",
    "#Potting the image using matplotlib\n",
    "plt.imshow(img)\n",
    "\n",
    "# The function image_to_string of pytesseract converts the text in the image into a string.\n",
    "print(tess.image_to_string('test.png'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "**Reading a multi-page PDF using pytesseract**\n",
    "\n",
    "To perform this, we need to include another python library **pdf2image**, we'll use this library to convert the pdf pages into images so that we can ask the pytesseract library to read them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "We'll traverse the whole pdf and create images from the pages of the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing pages of pdf in 'pages' variable\n",
    "pages = convert_from_path('review.pdf',500)\n",
    "\n",
    "# taking each page and saving as photo\n",
    "imageCounter = 1\n",
    "for page in pages:\n",
    "    fileName = \"page\" + str(imageCounter) + \".png\"\n",
    "    page.save(fileName,'PNG')\n",
    "    imageCounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "We'll read the images and then pass them to tesseract to do the OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the images and writing it to the output file\n",
    "fileName = 'Output.txt'\n",
    "with open(fileName,'a') as f:\n",
    "    for i in range(1,imageCounter):\n",
    "        text = str(tess.image_to_string('page' + str(i) + \".png\"))\n",
    "\n",
    "        text = text.replace('-\\n','')\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review of Real-time Visual Driver Distraction\n",
      "\n",
      "Detection Algorithms\n",
      "\n",
      "\n",
      "\n",
      "ChristerAhlIstrom\n",
      "\n",
      "christer.ahlstrom@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Katja Kircher\n",
      "\n",
      "katja.kircher@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Swedish National Road and Transport Research Institute (VTI),\n",
      "\n",
      "581 95 Linkoping, Sweden\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "\n",
      "\n",
      "Many incidents and crashes can be attributed to driver\n",
      "\n",
      "distraction, and it is essential to learn how to\n",
      "\n",
      "detectdistraction in order to develop _ efficient\n",
      "\n",
      "countermeasures. A number of distraction detection\n",
      "\n",
      "algorithms have been developed over the years, and the\n",
      "\n",
      "objective of this paper is to summarize available approaches\n",
      "\n",
      "and to describe these algorithms in a unified framework.The\n",
      "\n",
      "review is limited to real-time algorithms that are intended to\n",
      "\n",
      "detect visual distraction.\n",
      "\n",
      "\n",
      "\n",
      "Author Keywords\n",
      "\n",
      "Driver distraction, eye tracking, inattention, detection\n",
      "\n",
      "algorithm.\n",
      "\n",
      "\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "\n",
      "\n",
      "Driver inattention and distraction are major contributors to\n",
      "\n",
      "road incidents and crashes [1, 2]. Even so, drivers continue\n",
      "\n",
      "to engage themselves in distracting tasks such as using their\n",
      "\n",
      "mobile phones or navigation systems, eating, grooming and\n",
      "\n",
      "tending to their children. Advanced driver assistance\n",
      "\n",
      "systems may change this pattern by alerting the driver when\n",
      "\n",
      "his or her attention diverts from the road.\n",
      "\n",
      "\n",
      "\n",
      "Driver distraction can be defined as “the diversion of\n",
      "\n",
      "attention away from activities critical for safe driving\n",
      "\n",
      "toward a competing activity” [2]. This is a very general\n",
      "\n",
      "definition where the diversion of attention can be visual,\n",
      "\n",
      "auditive, physical or cognitive and where the competing\n",
      "\n",
      "activity can be anything from mobile phone usage to getting\n",
      "\n",
      "lost in thought. New advances in remote eye tracking\n",
      "\n",
      "technology provide a means to counteract distracted driving\n",
      "\n",
      "in real-time. Eye movements can be used to gain access to\n",
      "\n",
      "several types of distraction. For example, studies have\n",
      "\n",
      "shown that eye movements are sensitive not only to visual\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Permission to make digital or hard copies of all or part of this work for\n",
      "\n",
      "personal or classroom use is granted without fee provided that copies are\n",
      "\n",
      "not made or distributed for profit or commercial advantage and that copies\n",
      "\n",
      "bear this notice and the full citation on the first page. Copyrights for\n",
      "\n",
      "components of this work owned by others than ACM must be honored.\n",
      "\n",
      "Abstracting with credit 1s permitted. To copy otherwise, to republish, to\n",
      "\n",
      "post of servers or redistribute to lists requires prior specific permission\n",
      "\n",
      "and/or a fee.\n",
      "\n",
      "\n",
      "\n",
      "MB’10 August 24-27, 2010, Eindhoven, Netherlands\n",
      "\n",
      "©ACM 2010 ISBN: 978-1-60558-926-8/10/08...$10.00\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "distraction but also to auditory secondary tasks [3-5].\n",
      "\n",
      "\n",
      "\n",
      "There are numerous performance indicators that are based\n",
      "\n",
      "on longitudinal and lateral vehicle control dynamicswhich\n",
      "\n",
      "correlate with visual as well as cognitive task demands] 68]. These include steering wheel reversal rate, average\n",
      "\n",
      "proportion of high frequency steering, brake reaction time,\n",
      "\n",
      "steering entropy, throttle hold, variability in lateral position,\n",
      "\n",
      "number of lane exceedences, time- and distance headway\n",
      "\n",
      "and time to collision. Even though many of these\n",
      "\n",
      "performance indicators seem to be promising secondary\n",
      "\n",
      "task identifiers, we restrict this survey to gaze based\n",
      "\n",
      "distraction detection algorithms.The objective of this paper\n",
      "\n",
      "is thus to summarize available real-time gaze based\n",
      "\n",
      "approaches for measuring visual driver distraction and to\n",
      "\n",
      "describe these algorithms in a unified framework. Since the\n",
      "\n",
      "focus of this review is on real-time assessment of visual\n",
      "\n",
      "distraction, many after-the-fact methods based on reaction\n",
      "\n",
      "times, secondary task performance and_ corrective\n",
      "\n",
      "manoeuvres are left out. A survey of the effects, in contrast\n",
      "\n",
      "to the prediction, of driver distraction can be found in\n",
      "\n",
      "Young et al. [9].\n",
      "\n",
      "\n",
      "\n",
      "PRINCIPLES OF DRIVER DISTRACTION DETECTION\n",
      "\n",
      "\n",
      "\n",
      "A schematic overview summarizing the structure of most\n",
      "\n",
      "driver distraction detection algorithms is illustrated in\n",
      "\n",
      "Figure 1. The basis for all algorithms is measures registered\n",
      "\n",
      "in real-time during driving. They can stem from the driver\n",
      "\n",
      "(driver behaviour), like eye movements or hand\n",
      "\n",
      "movements, or they can be logged from the vehicle (driving\n",
      "\n",
      "behaviour), like speed or lateral position. Furthermore,\n",
      "\n",
      "situational variables like time and position can be used\n",
      "\n",
      "(other data). Certain features of these data, like gaze\n",
      "\n",
      "direction, steering entropy or others are extracted and\n",
      "\n",
      "possibly fused in order to arrive at a continuous measure of\n",
      "\n",
      "the driver’s distraction level. This output is then used to\n",
      "\n",
      "classify the driver’s state of attention. For most algorithms\n",
      "\n",
      "these states are visually distracted vs. not visually\n",
      "\n",
      "distracted.\n",
      "\n",
      "\n",
      "\n",
      "Field Relevant for Driving\n",
      "\n",
      "\n",
      "\n",
      "Common for all eye or head movement based distraction\n",
      "\n",
      "algorithms is that they use off-road glances as the basic\n",
      "\n",
      "source of information. The idea is to define a field relevantdistraction detection algorithm\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "—\n",
      "\n",
      "5\n",
      "\n",
      "2\n",
      "\n",
      "— -——_—\n",
      "\n",
      "Oo &\n",
      "\n",
      ">\n",
      "\n",
      "‘= oO\n",
      "\n",
      "oa\n",
      "\n",
      "<\n",
      "\n",
      "2\n",
      "\n",
      "wn = 6\n",
      "\n",
      "wo > gS c¢\n",
      "\n",
      "5 op -S ® 2\n",
      "\n",
      "a —— ___ dg o __ a\n",
      "\n",
      "© se wo >\n",
      "\n",
      "wy = Ow = —\n",
      "\n",
      "£ >o 2 a\n",
      "\n",
      "&\n",
      "\n",
      "@\n",
      "\n",
      "—\n",
      "\n",
      "ros)\n",
      "\n",
      "~Y\n",
      "\n",
      "so\n",
      "\n",
      "ra —<$—$————$ >\n",
      "\n",
      "@\n",
      "\n",
      "<=\n",
      "\n",
      "~\n",
      "\n",
      "oO\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "classification\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "warning algorithm\n",
      "\n",
      "\n",
      "\n",
      "not\n",
      "\n",
      "nn > distracted\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "no warning\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "-—- - Se FH —\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "| I\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Bo warning\n",
      "\n",
      "Ww\n",
      "\n",
      "\n",
      "\n",
      "——- -— = — > i\n",
      "\n",
      "oa)\n",
      "\n",
      "WN\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 1. A schematic overview of the common features of most driver distraction detection algorithms and mitigation systems.\n",
      "\n",
      "\n",
      "\n",
      "for driving, which is basically the area where the driver is\n",
      "\n",
      "looking when he or she is driving. If a world model is not\n",
      "\n",
      "available, the field relevant for driving can, for example, be\n",
      "\n",
      "defined as a circle [10-12] or a rectangle [13], see Figure 2.\n",
      "\n",
      "It is also possible to select different shapes. In Kircher et al.\n",
      "\n",
      "[14], a circle where the lower part was removed was used\n",
      "\n",
      "so that the dashboard would not be included in the field\n",
      "\n",
      "relevant for driving. Since there is no information about\n",
      "\n",
      "where the driver is looking in the real world, the selected\n",
      "\n",
      "field relevant for driving needs to be positioned in the real\n",
      "\n",
      "world based on statistics of where the driver has been\n",
      "\n",
      "looking. This is often done by centering the selected shape\n",
      "\n",
      "around the largest peak in the distribution of recent gazes.\n",
      "\n",
      "When enough gaze data has been acquired, it is also\n",
      "\n",
      "possible to define more than one zone based on the\n",
      "\n",
      "distribution of the data. For example, Kutila et al. [15] uses\n",
      "\n",
      "four zones (road ahead, windscreen and left/right exterior\n",
      "\n",
      "mirror.\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 2. Examples of three different approaches to select\n",
      "\n",
      "the field relevant from driving. The leftmost plot show a\n",
      "\n",
      "circular area, the middle plot shows a rectangular area and\n",
      "\n",
      "the rightmost plot shows an area that is defined based on the\n",
      "\n",
      "cockpit of the car.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "If the eye tracking systems allows a world model to be\n",
      "\n",
      "used, the field relevant for driving can be defined based on\n",
      "\n",
      "different zones related to the interior of the car. This\n",
      "\n",
      "approach is used by Pohl et al. [16] and Kircher et al. [14],\n",
      "\n",
      "see Figure 2. In the latter of these two, the field relevant for\n",
      "\n",
      "driving is defined as the intersection between a viewing\n",
      "\n",
      "cone of 90 degrees and the vehicle’s windows. This means\n",
      "\n",
      "that the circular field relevant for driving concept is\n",
      "\n",
      "expanded with information about the design of the car.\n",
      "\n",
      "\n",
      "\n",
      "Distraction Estimation\n",
      "\n",
      "\n",
      "\n",
      "Glances away from the road ahead are usually defined as\n",
      "\n",
      "glances residing outside the field relevant for driving. The\n",
      "\n",
      "duration of these glances away from the road ahead is the\n",
      "\n",
      "basic source of information that all visual distraction\n",
      "\n",
      "detection algorithms to date are based upon. If the driver is\n",
      "\n",
      "looking away from the road too often or for too long, the\n",
      "\n",
      "driver is considered distracted.\n",
      "\n",
      "\n",
      "\n",
      "The mappings that transform glances away from the road to\n",
      "\n",
      "a continuous distraction estimate are often very similar. For\n",
      "\n",
      "example, Zhang et al. [13] used the average duration of\n",
      "\n",
      "glances away from the road in a 4.3-second wide sliding\n",
      "\n",
      "window, Donmez et al. [17] used a weighted sum of the\n",
      "\n",
      "current glance and the average glance duration in a 3second sliding window and Victor [10] used the percentage\n",
      "\n",
      "of on-road gaze data points in a 60-second sliding window.\n",
      "\n",
      "A slightly different approach is to use a buffer [14] or a\n",
      "\n",
      "counter [11] that changes its value when the driver looks\n",
      "\n",
      "away. Here the counter/buffer reaches amaximum/minimum value when the driver is judged to be\n",
      "\n",
      "too distracted.\n",
      "\n",
      "\n",
      "\n",
      "So far, there has been a direct link from the FRD via the\n",
      "\n",
      "glance duration to the estimated distraction level in the\n",
      "\n",
      "sense that all gazes have the same weight, regardless of\n",
      "\n",
      "where the gaze is directed. However, it is possible to make\n",
      "\n",
      "this link fuzzier by changing the weight as a function of\n",
      "\n",
      "where the gaze is directed. One idea is thus to penalize\n",
      "\n",
      "glances that are far away from the road centre. In the\n",
      "\n",
      "SafeTE project [12], this was done by the so-called\n",
      "\n",
      "eccentricity function E(a) = 6.5758—-1/(0.001*a + 0.152).\n",
      "\n",
      "This is basically a weighting function that favours glances\n",
      "\n",
      "close to the road centre while penalizing glances with a\n",
      "\n",
      "large gaze direction angle. The equation is based on a study\n",
      "\n",
      "by Lamble et al. [18] and is related to visual behaviour and\n",
      "\n",
      "brake response when a lead vehicle suddenly starts to\n",
      "\n",
      "decelerate. In cases where a world model is available, it is\n",
      "\n",
      "possible to use different weights on different objects [14,\n",
      "\n",
      "16]. For example, the rear view mirrors and_ the\n",
      "\n",
      "speedometer could have a higher weight as compared to the\n",
      "\n",
      "field relevant for driving but lower than the middle console\n",
      "\n",
      "or the glove compartment. Higher weights in this context\n",
      "\n",
      "mean that the distraction estimate will increase faster while\n",
      "\n",
      "lower weights have the opposite effect.Other combinations\n",
      "\n",
      "of the distraction estimation functions mentioned above, 1.e.\n",
      "\n",
      "glance duration, glance history and eccentricity,has also\n",
      "\n",
      "been suggested [19].\n",
      "\n",
      "\n",
      "\n",
      "Distraction Decision\n",
      "\n",
      "\n",
      "\n",
      "The continuous distraction estimate needs to be mapped to a\n",
      "\n",
      "decision whether the driver is distracted or not. Basically,\n",
      "\n",
      "the driver enters the distracted state when a threshold is\n",
      "\n",
      "reached and returns to the attentive state when some criteria\n",
      "\n",
      "are fulfilled. The main difference between different\n",
      "\n",
      "approaches is how to leave the distracted state. One\n",
      "\n",
      "approach is to require that the driver is looking forward for\n",
      "\n",
      "some minimum time before he or she is considered to be\n",
      "\n",
      "attentive [14, 16, 17]. The other approach is that it is\n",
      "\n",
      "enough for the driver to look back at the road to be\n",
      "\n",
      "considered fully attentive [11].\n",
      "\n",
      "\n",
      "\n",
      "Inhibition Criteria\n",
      "\n",
      "\n",
      "\n",
      "A distraction detection algorithm determines whether a\n",
      "\n",
      "driver is distracted or not, but when and in which way the\n",
      "\n",
      "driver will be warned for distraction is determined by a\n",
      "\n",
      "warning strategy. Information about different warning\n",
      "\n",
      "strategies is out of the scope of this review. More\n",
      "\n",
      "information can be found in, for example, Donmez et al.\n",
      "\n",
      "[20]. However, there are situations when it is not suitable to\n",
      "\n",
      "give distraction warnings. For instance, if the driver is\n",
      "\n",
      "braking hard he or she is probably aware of the situation\n",
      "\n",
      "and should not be disturbed by a warning. For this reason,\n",
      "\n",
      "certain criteria can be set up to inhibit warnings. Common\n",
      "\n",
      "criteria include [21]:\n",
      "\n",
      "\n",
      "\n",
      "e Speed: Below 50 km/h gaze behaviour is not very\n",
      "\n",
      "uniform. The gaze is often outside the FRD without the\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "driver being distracted.\n",
      "\n",
      "\n",
      "\n",
      "e Direction indicators: Changing lanes and turning can\n",
      "\n",
      "include planned glances outside the FRD.\n",
      "\n",
      "\n",
      "\n",
      "e Reverse gear: Reverse engaged means that the driver\n",
      "\n",
      "should look over the shoulder.\n",
      "\n",
      "\n",
      "\n",
      "e Brake pedal: No warning should be given while driver is\n",
      "\n",
      "braking, in order not to interfere with critical driving\n",
      "\n",
      "manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Steering wheel angle: No warning should be given while\n",
      "\n",
      "the driver is engaged in substantial changes of direction,\n",
      "\n",
      "in order not to interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Lateral acceleration: No warning should be given when\n",
      "\n",
      "the vehicle makes strong movements, in order not to\n",
      "\n",
      "interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "IMPROVEMENTS AND FUTURE RESEARCH\n",
      "\n",
      "\n",
      "\n",
      "Available algorithms for eye tracking based driver\n",
      "\n",
      "distraction detection attempt to detect visual distraction.\n",
      "\n",
      "All algorithms can be fitted in a common framework;\n",
      "\n",
      "determine if the driver is looking at the road or not, convert\n",
      "\n",
      "this information into a continuous estimate of (visual)\n",
      "\n",
      "distraction and finally use some rule, often a threshold, to\n",
      "\n",
      "determine if the estimated level of distraction should be\n",
      "\n",
      "considered distracted or attentive. The main limitation of\n",
      "\n",
      "these approaches is that they do not take the current traffic\n",
      "\n",
      "situation into account. This could be done by allowing the\n",
      "\n",
      "field relevant for driving to change dynamically over time.\n",
      "\n",
      "Future research is needed to (a) determine the optimal field\n",
      "\n",
      "relevant for driving for different traffic situations and traffic\n",
      "\n",
      "environments and (b) develop technology to be able to\n",
      "\n",
      "measure the current traffic situation and _ traffic\n",
      "\n",
      "environment.\n",
      "\n",
      "\n",
      "\n",
      "Only one of the available algorithms (percent road centre)\n",
      "\n",
      "was prepared in order to detect internal distraction.\n",
      "\n",
      "Suggested measures of internal distraction are based on the\n",
      "\n",
      "concentration of gazes towards the road centre area, which\n",
      "\n",
      "is higher when the driver is lost in thought. It has been\n",
      "\n",
      "suggested that other eye movements such as saccades and\n",
      "\n",
      "microsaccades could be indicative of workload or\n",
      "\n",
      "inattention. Future research is needed to (a) investigate eye\n",
      "\n",
      "movement physiology during driving, (b) develop remote\n",
      "\n",
      "eye tracking technology with higher accuracy so that these\n",
      "\n",
      "small and fast eye movements can be measured, and (c)\n",
      "\n",
      "develop algorithms that reliably and accurately detect\n",
      "\n",
      "different types of eye movements like fixations, saccades\n",
      "\n",
      "and smooth pursuit from the continuous data stream.\n",
      "\n",
      "\n",
      "\n",
      "Other distraction indicators such as lateral and longitudinal\n",
      "\n",
      "control parameters seem to be very task and situation\n",
      "\n",
      "dependent, and it is questionable whether they can be used\n",
      "\n",
      "in a general purpose driver distraction detection algorithm.\n",
      "\n",
      "Future research includes fusion of several data sources,\n",
      "\n",
      "including situational variables, so that the appropriate set of\n",
      "\n",
      "performance indicators is used at exactly the right place at\n",
      "\n",
      "the right time. Even though it might be impossible to\n",
      "\n",
      "replace eye movement related indicators completely withdriving related parameters, it would be very valuable to be\n",
      "\n",
      "able to fall back on this type of data when eye tracking is\n",
      "\n",
      "lost.\n",
      "\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "\n",
      "\n",
      "1. Gordon, C.P., Crash Studies of Driver Distraction, in\n",
      "\n",
      "Driver Distraction: Theory, effect and mitigation, M.A.\n",
      "\n",
      "Regan, J.D. Lee, and K.L. Young, Editors. CRC Press,\n",
      "\n",
      "Taylor & Francis Group, London (2009), 281-304.\n",
      "\n",
      "\n",
      "\n",
      "2. Lee, J.D., K.L. Young, and M.A. Regan, Defining\n",
      "\n",
      "Driver Distraction, in Driver Distraction: Theory, effect\n",
      "\n",
      "and mitigation, M.A. Regan, J.D. Lee, and K.L. Young,\n",
      "\n",
      "Editors. CRC Press, Taylor & Francis Group London\n",
      "\n",
      "(2009), 31-40.\n",
      "\n",
      "\n",
      "\n",
      "3. Recarte, M.A. and L.M. Nunes, Effects of verbal and\n",
      "\n",
      "spatial-imagery tasks on eye fixations while driving.\n",
      "\n",
      "Journal of Experimental Psychology: Applied, 6,1\n",
      "\n",
      "(2000), 31-43.\n",
      "\n",
      "\n",
      "\n",
      "4. Victor, T.W., J.L. Harbluk, and J. Engstrém, Sensitivity\n",
      "\n",
      "of eye-movement measures to in-vehicle task difficulty.\n",
      "\n",
      "Transportation Research Part F: Traffic Psychology and\n",
      "\n",
      "Behaviour, &, 2 (2005), 167-190.\n",
      "\n",
      "\n",
      "\n",
      "5. Engstrém, J., E. Johansson, and J. Ostlund, Effects of\n",
      "\n",
      "visual and cognitive load in real and_ simulated\n",
      "\n",
      "motorway driving. Transportation Research Part F:\n",
      "\n",
      "Traffic Psychology and Behaviour, 8, 2 (2005) 97-120.\n",
      "\n",
      "\n",
      "\n",
      "6. Green, P. and R. Shah, Task time and glance measures\n",
      "\n",
      "of the use of telematics: A tabular summary of the\n",
      "\n",
      "literature, in SAfety VEhicles using adaptive Interface\n",
      "\n",
      "Technology (2004), UMTRI: Ann Arbor.\n",
      "\n",
      "\n",
      "\n",
      "7. Zylstra, B., et al., Driving performance for dialing,\n",
      "\n",
      "radio tuning, and destination entry while driving\n",
      "\n",
      "straight roads. The University of Michigan\n",
      "\n",
      "Transportation Research Institute: Ann Arbor, MI\n",
      "\n",
      "(2003).\n",
      "\n",
      "\n",
      "\n",
      "8. Ostlund, J., et al., Driving performance assessment methods and metrics, in EU Deliverable, Adaptive\n",
      "\n",
      "Integrated Driver-Vehicle Interface Project (AIDE).\n",
      "\n",
      "(2005).\n",
      "\n",
      "\n",
      "\n",
      "9. Young, L.K., M.A. Regan, and J.D. Lee, Measuring the\n",
      "\n",
      "effects of driver distraction: Direct driving performance\n",
      "\n",
      "methods and measures, in Driver distraction: Theory,\n",
      "\n",
      "effects and mitigation, M.A. Regan, J.D. Lee, and K.L.\n",
      "\n",
      "Young, Editors. CRC PRess London (2009), 85-105.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "10. Victor, T.W., Keeping eye and mind on the road, in\n",
      "\n",
      "Dept of Psychology. Digital Comprehensive Summaries\n",
      "\n",
      "of Uppsala Dissertations from the Faculty of Social\n",
      "\n",
      "Sciences 9, Uppsala University: Uppsala (2005).\n",
      "\n",
      "\n",
      "\n",
      "11. Fletcher, L. and A. Zelinsky. Driver state monitoring to\n",
      "\n",
      "mitigate distraction. in International Conference on\n",
      "\n",
      "Driver Distraction. Sydney, Australia(2005).\n",
      "\n",
      "\n",
      "\n",
      "12. Engstrom, J. and S. Mardh, SafeTE Final Report(2007).\n",
      "\n",
      "\n",
      "\n",
      "13. Zhang, H., M. Smith, and R. Dufour, A Final Report of\n",
      "\n",
      "SAfety VEhicles using adaptive Interface Technology\n",
      "\n",
      "(Phase II: Task 7C): Visual Distraction (2008).\n",
      "\n",
      "\n",
      "\n",
      "14. Kircher, K. and C. Ahlstrom, Issues related to the driver\n",
      "\n",
      "distraction detection algorithm AttenD, in _ Ist\n",
      "\n",
      "International Conference on Driver Distraction and\n",
      "\n",
      "Inattention, Gothenburg, Sweden (2009).\n",
      "\n",
      "\n",
      "\n",
      "15. Kutila, M., et al. Driver Distraction Detection with a\n",
      "\n",
      "Camera Vision System. in Proceedings of the [EEE\n",
      "\n",
      "International Conference on Image Processing. San\n",
      "\n",
      "Antonio, Texas, US (2007).\n",
      "\n",
      "\n",
      "\n",
      "16.Pohl, J., W. Birk, and L. Westervall, A driverdistraction-based lane-keeping assistance system.\n",
      "\n",
      "Proceedings of the Institution of Mechanical Engineers\n",
      "\n",
      "Part I-Journal of Systems and Control Engineering, 221,\n",
      "\n",
      "14 (2007), 541-552.\n",
      "\n",
      "\n",
      "\n",
      "17.Donmez, B., L.N. Boyle, and J.D. Lee, Safety\n",
      "\n",
      "implications of providing real-time feedback to\n",
      "\n",
      "distracted drivers.Accident Analysis and Prevention, 39,\n",
      "\n",
      "3 (2007), 581-590.\n",
      "\n",
      "\n",
      "\n",
      "18. Lamble, D., M. Laakso, and H. Summala, Detection\n",
      "\n",
      "thresholds in car following situations and peripheral\n",
      "\n",
      "vision: implications for positioning of visually\n",
      "\n",
      "demanding in-car displays. Ergonomics, 42, 6, (1999),\n",
      "\n",
      "807-815.\n",
      "\n",
      "\n",
      "\n",
      "19. Liang, Y., Detecting Driver Distraction. University of\n",
      "\n",
      "Iowa (2009).\n",
      "\n",
      "\n",
      "\n",
      "20.Donmez, B., L.N. Boyle, and J.D. Lee, Designing\n",
      "\n",
      "feedback to mitigate distraction, in Driver distraction:\n",
      "\n",
      "Theory, effects and mitigation, M.A. Regan, J.D. Lee,\n",
      "\n",
      "and K.L. Young, Editors. CRC PRess London (2009),\n",
      "\n",
      "519-531.\n",
      "\n",
      "\n",
      "\n",
      "21. Kircher, K., A. Kircher, and F. Claezon, Distraction and\n",
      "\n",
      "drowsiness. A field study. VTI: Linképing (2009).Review of Real-time Visual Driver Distraction\n",
      "\n",
      "Detection Algorithms\n",
      "\n",
      "\n",
      "\n",
      "ChristerAhlIstrom\n",
      "\n",
      "christer.ahlstrom@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Katja Kircher\n",
      "\n",
      "katja.kircher@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Swedish National Road and Transport Research Institute (VTI),\n",
      "\n",
      "581 95 Linkoping, Sweden\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "\n",
      "\n",
      "Many incidents and crashes can be attributed to driver\n",
      "\n",
      "distraction, and it is essential to learn how to\n",
      "\n",
      "detectdistraction in order to develop _ efficient\n",
      "\n",
      "countermeasures. A number of distraction detection\n",
      "\n",
      "algorithms have been developed over the years, and the\n",
      "\n",
      "objective of this paper is to summarize available approaches\n",
      "\n",
      "and to describe these algorithms in a unified framework.The\n",
      "\n",
      "review is limited to real-time algorithms that are intended to\n",
      "\n",
      "detect visual distraction.\n",
      "\n",
      "\n",
      "\n",
      "Author Keywords\n",
      "\n",
      "Driver distraction, eye tracking, inattention, detection\n",
      "\n",
      "algorithm.\n",
      "\n",
      "\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "\n",
      "\n",
      "Driver inattention and distraction are major contributors to\n",
      "\n",
      "road incidents and crashes [1, 2]. Even so, drivers continue\n",
      "\n",
      "to engage themselves in distracting tasks such as using their\n",
      "\n",
      "mobile phones or navigation systems, eating, grooming and\n",
      "\n",
      "tending to their children. Advanced driver assistance\n",
      "\n",
      "systems may change this pattern by alerting the driver when\n",
      "\n",
      "his or her attention diverts from the road.\n",
      "\n",
      "\n",
      "\n",
      "Driver distraction can be defined as “the diversion of\n",
      "\n",
      "attention away from activities critical for safe driving\n",
      "\n",
      "toward a competing activity” [2]. This is a very general\n",
      "\n",
      "definition where the diversion of attention can be visual,\n",
      "\n",
      "auditive, physical or cognitive and where the competing\n",
      "\n",
      "activity can be anything from mobile phone usage to getting\n",
      "\n",
      "lost in thought. New advances in remote eye tracking\n",
      "\n",
      "technology provide a means to counteract distracted driving\n",
      "\n",
      "in real-time. Eye movements can be used to gain access to\n",
      "\n",
      "several types of distraction. For example, studies have\n",
      "\n",
      "shown that eye movements are sensitive not only to visual\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Permission to make digital or hard copies of all or part of this work for\n",
      "\n",
      "personal or classroom use is granted without fee provided that copies are\n",
      "\n",
      "not made or distributed for profit or commercial advantage and that copies\n",
      "\n",
      "bear this notice and the full citation on the first page. Copyrights for\n",
      "\n",
      "components of this work owned by others than ACM must be honored.\n",
      "\n",
      "Abstracting with credit 1s permitted. To copy otherwise, to republish, to\n",
      "\n",
      "post of servers or redistribute to lists requires prior specific permission\n",
      "\n",
      "and/or a fee.\n",
      "\n",
      "\n",
      "\n",
      "MB’10 August 24-27, 2010, Eindhoven, Netherlands\n",
      "\n",
      "©ACM 2010 ISBN: 978-1-60558-926-8/10/08...$10.00\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "distraction but also to auditory secondary tasks [3-5].\n",
      "\n",
      "\n",
      "\n",
      "There are numerous performance indicators that are based\n",
      "\n",
      "on longitudinal and lateral vehicle control dynamicswhich\n",
      "\n",
      "correlate with visual as well as cognitive task demands] 68]. These include steering wheel reversal rate, average\n",
      "\n",
      "proportion of high frequency steering, brake reaction time,\n",
      "\n",
      "steering entropy, throttle hold, variability in lateral position,\n",
      "\n",
      "number of lane exceedences, time- and distance headway\n",
      "\n",
      "and time to collision. Even though many of these\n",
      "\n",
      "performance indicators seem to be promising secondary\n",
      "\n",
      "task identifiers, we restrict this survey to gaze based\n",
      "\n",
      "distraction detection algorithms.The objective of this paper\n",
      "\n",
      "is thus to summarize available real-time gaze based\n",
      "\n",
      "approaches for measuring visual driver distraction and to\n",
      "\n",
      "describe these algorithms in a unified framework. Since the\n",
      "\n",
      "focus of this review is on real-time assessment of visual\n",
      "\n",
      "distraction, many after-the-fact methods based on reaction\n",
      "\n",
      "times, secondary task performance and_ corrective\n",
      "\n",
      "manoeuvres are left out. A survey of the effects, in contrast\n",
      "\n",
      "to the prediction, of driver distraction can be found in\n",
      "\n",
      "Young et al. [9].\n",
      "\n",
      "\n",
      "\n",
      "PRINCIPLES OF DRIVER DISTRACTION DETECTION\n",
      "\n",
      "\n",
      "\n",
      "A schematic overview summarizing the structure of most\n",
      "\n",
      "driver distraction detection algorithms is illustrated in\n",
      "\n",
      "Figure 1. The basis for all algorithms is measures registered\n",
      "\n",
      "in real-time during driving. They can stem from the driver\n",
      "\n",
      "(driver behaviour), like eye movements or hand\n",
      "\n",
      "movements, or they can be logged from the vehicle (driving\n",
      "\n",
      "behaviour), like speed or lateral position. Furthermore,\n",
      "\n",
      "situational variables like time and position can be used\n",
      "\n",
      "(other data). Certain features of these data, like gaze\n",
      "\n",
      "direction, steering entropy or others are extracted and\n",
      "\n",
      "possibly fused in order to arrive at a continuous measure of\n",
      "\n",
      "the driver’s distraction level. This output is then used to\n",
      "\n",
      "classify the driver’s state of attention. For most algorithms\n",
      "\n",
      "these states are visually distracted vs. not visually\n",
      "\n",
      "distracted.\n",
      "\n",
      "\n",
      "\n",
      "Field Relevant for Driving\n",
      "\n",
      "\n",
      "\n",
      "Common for all eye or head movement based distraction\n",
      "\n",
      "algorithms is that they use off-road glances as the basic\n",
      "\n",
      "source of information. The idea is to define a field relevantdistraction detection algorithm\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "—\n",
      "\n",
      "5\n",
      "\n",
      "2\n",
      "\n",
      "— -——_—\n",
      "\n",
      "Oo &\n",
      "\n",
      ">\n",
      "\n",
      "‘= oO\n",
      "\n",
      "oa\n",
      "\n",
      "<\n",
      "\n",
      "2\n",
      "\n",
      "wn = 6\n",
      "\n",
      "wo > gS c¢\n",
      "\n",
      "5 op -S ® 2\n",
      "\n",
      "a —— ___ dg o __ a\n",
      "\n",
      "© se wo >\n",
      "\n",
      "wy = Ow = —\n",
      "\n",
      "£ >o 2 a\n",
      "\n",
      "&\n",
      "\n",
      "@\n",
      "\n",
      "—\n",
      "\n",
      "ros)\n",
      "\n",
      "~Y\n",
      "\n",
      "so\n",
      "\n",
      "ra —<$—$————$ >\n",
      "\n",
      "@\n",
      "\n",
      "<=\n",
      "\n",
      "~\n",
      "\n",
      "oO\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "classification\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "warning algorithm\n",
      "\n",
      "\n",
      "\n",
      "not\n",
      "\n",
      "nn > distracted\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "no warning\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "-—- - Se FH —\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "| I\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Bo warning\n",
      "\n",
      "Ww\n",
      "\n",
      "\n",
      "\n",
      "——- -— = — > i\n",
      "\n",
      "oa)\n",
      "\n",
      "WN\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 1. A schematic overview of the common features of most driver distraction detection algorithms and mitigation systems.\n",
      "\n",
      "\n",
      "\n",
      "for driving, which is basically the area where the driver is\n",
      "\n",
      "looking when he or she is driving. If a world model is not\n",
      "\n",
      "available, the field relevant for driving can, for example, be\n",
      "\n",
      "defined as a circle [10-12] or a rectangle [13], see Figure 2.\n",
      "\n",
      "It is also possible to select different shapes. In Kircher et al.\n",
      "\n",
      "[14], a circle where the lower part was removed was used\n",
      "\n",
      "so that the dashboard would not be included in the field\n",
      "\n",
      "relevant for driving. Since there is no information about\n",
      "\n",
      "where the driver is looking in the real world, the selected\n",
      "\n",
      "field relevant for driving needs to be positioned in the real\n",
      "\n",
      "world based on statistics of where the driver has been\n",
      "\n",
      "looking. This is often done by centering the selected shape\n",
      "\n",
      "around the largest peak in the distribution of recent gazes.\n",
      "\n",
      "When enough gaze data has been acquired, it is also\n",
      "\n",
      "possible to define more than one zone based on the\n",
      "\n",
      "distribution of the data. For example, Kutila et al. [15] uses\n",
      "\n",
      "four zones (road ahead, windscreen and left/right exterior\n",
      "\n",
      "mirror.\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 2. Examples of three different approaches to select\n",
      "\n",
      "the field relevant from driving. The leftmost plot show a\n",
      "\n",
      "circular area, the middle plot shows a rectangular area and\n",
      "\n",
      "the rightmost plot shows an area that is defined based on the\n",
      "\n",
      "cockpit of the car.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "If the eye tracking systems allows a world model to be\n",
      "\n",
      "used, the field relevant for driving can be defined based on\n",
      "\n",
      "different zones related to the interior of the car. This\n",
      "\n",
      "approach is used by Pohl et al. [16] and Kircher et al. [14],\n",
      "\n",
      "see Figure 2. In the latter of these two, the field relevant for\n",
      "\n",
      "driving is defined as the intersection between a viewing\n",
      "\n",
      "cone of 90 degrees and the vehicle’s windows. This means\n",
      "\n",
      "that the circular field relevant for driving concept is\n",
      "\n",
      "expanded with information about the design of the car.\n",
      "\n",
      "\n",
      "\n",
      "Distraction Estimation\n",
      "\n",
      "\n",
      "\n",
      "Glances away from the road ahead are usually defined as\n",
      "\n",
      "glances residing outside the field relevant for driving. The\n",
      "\n",
      "duration of these glances away from the road ahead is the\n",
      "\n",
      "basic source of information that all visual distraction\n",
      "\n",
      "detection algorithms to date are based upon. If the driver is\n",
      "\n",
      "looking away from the road too often or for too long, the\n",
      "\n",
      "driver is considered distracted.\n",
      "\n",
      "\n",
      "\n",
      "The mappings that transform glances away from the road to\n",
      "\n",
      "a continuous distraction estimate are often very similar. For\n",
      "\n",
      "example, Zhang et al. [13] used the average duration of\n",
      "\n",
      "glances away from the road in a 4.3-second wide sliding\n",
      "\n",
      "window, Donmez et al. [17] used a weighted sum of the\n",
      "\n",
      "current glance and the average glance duration in a 3second sliding window and Victor [10] used the percentage\n",
      "\n",
      "of on-road gaze data points in a 60-second sliding window.\n",
      "\n",
      "A slightly different approach is to use a buffer [14] or a\n",
      "\n",
      "counter [11] that changes its value when the driver looks\n",
      "\n",
      "away. Here the counter/buffer reaches amaximum/minimum value when the driver is judged to be\n",
      "\n",
      "too distracted.\n",
      "\n",
      "\n",
      "\n",
      "So far, there has been a direct link from the FRD via the\n",
      "\n",
      "glance duration to the estimated distraction level in the\n",
      "\n",
      "sense that all gazes have the same weight, regardless of\n",
      "\n",
      "where the gaze is directed. However, it is possible to make\n",
      "\n",
      "this link fuzzier by changing the weight as a function of\n",
      "\n",
      "where the gaze is directed. One idea is thus to penalize\n",
      "\n",
      "glances that are far away from the road centre. In the\n",
      "\n",
      "SafeTE project [12], this was done by the so-called\n",
      "\n",
      "eccentricity function E(a) = 6.5758—-1/(0.001*a + 0.152).\n",
      "\n",
      "This is basically a weighting function that favours glances\n",
      "\n",
      "close to the road centre while penalizing glances with a\n",
      "\n",
      "large gaze direction angle. The equation is based on a study\n",
      "\n",
      "by Lamble et al. [18] and is related to visual behaviour and\n",
      "\n",
      "brake response when a lead vehicle suddenly starts to\n",
      "\n",
      "decelerate. In cases where a world model is available, it is\n",
      "\n",
      "possible to use different weights on different objects [14,\n",
      "\n",
      "16]. For example, the rear view mirrors and_ the\n",
      "\n",
      "speedometer could have a higher weight as compared to the\n",
      "\n",
      "field relevant for driving but lower than the middle console\n",
      "\n",
      "or the glove compartment. Higher weights in this context\n",
      "\n",
      "mean that the distraction estimate will increase faster while\n",
      "\n",
      "lower weights have the opposite effect.Other combinations\n",
      "\n",
      "of the distraction estimation functions mentioned above, 1.e.\n",
      "\n",
      "glance duration, glance history and eccentricity,has also\n",
      "\n",
      "been suggested [19].\n",
      "\n",
      "\n",
      "\n",
      "Distraction Decision\n",
      "\n",
      "\n",
      "\n",
      "The continuous distraction estimate needs to be mapped to a\n",
      "\n",
      "decision whether the driver is distracted or not. Basically,\n",
      "\n",
      "the driver enters the distracted state when a threshold is\n",
      "\n",
      "reached and returns to the attentive state when some criteria\n",
      "\n",
      "are fulfilled. The main difference between different\n",
      "\n",
      "approaches is how to leave the distracted state. One\n",
      "\n",
      "approach is to require that the driver is looking forward for\n",
      "\n",
      "some minimum time before he or she is considered to be\n",
      "\n",
      "attentive [14, 16, 17]. The other approach is that it is\n",
      "\n",
      "enough for the driver to look back at the road to be\n",
      "\n",
      "considered fully attentive [11].\n",
      "\n",
      "\n",
      "\n",
      "Inhibition Criteria\n",
      "\n",
      "\n",
      "\n",
      "A distraction detection algorithm determines whether a\n",
      "\n",
      "driver is distracted or not, but when and in which way the\n",
      "\n",
      "driver will be warned for distraction is determined by a\n",
      "\n",
      "warning strategy. Information about different warning\n",
      "\n",
      "strategies is out of the scope of this review. More\n",
      "\n",
      "information can be found in, for example, Donmez et al.\n",
      "\n",
      "[20]. However, there are situations when it is not suitable to\n",
      "\n",
      "give distraction warnings. For instance, if the driver is\n",
      "\n",
      "braking hard he or she is probably aware of the situation\n",
      "\n",
      "and should not be disturbed by a warning. For this reason,\n",
      "\n",
      "certain criteria can be set up to inhibit warnings. Common\n",
      "\n",
      "criteria include [21]:\n",
      "\n",
      "\n",
      "\n",
      "e Speed: Below 50 km/h gaze behaviour is not very\n",
      "\n",
      "uniform. The gaze is often outside the FRD without the\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "driver being distracted.\n",
      "\n",
      "\n",
      "\n",
      "e Direction indicators: Changing lanes and turning can\n",
      "\n",
      "include planned glances outside the FRD.\n",
      "\n",
      "\n",
      "\n",
      "e Reverse gear: Reverse engaged means that the driver\n",
      "\n",
      "should look over the shoulder.\n",
      "\n",
      "\n",
      "\n",
      "e Brake pedal: No warning should be given while driver is\n",
      "\n",
      "braking, in order not to interfere with critical driving\n",
      "\n",
      "manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Steering wheel angle: No warning should be given while\n",
      "\n",
      "the driver is engaged in substantial changes of direction,\n",
      "\n",
      "in order not to interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Lateral acceleration: No warning should be given when\n",
      "\n",
      "the vehicle makes strong movements, in order not to\n",
      "\n",
      "interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "IMPROVEMENTS AND FUTURE RESEARCH\n",
      "\n",
      "\n",
      "\n",
      "Available algorithms for eye tracking based driver\n",
      "\n",
      "distraction detection attempt to detect visual distraction.\n",
      "\n",
      "All algorithms can be fitted in a common framework;\n",
      "\n",
      "determine if the driver is looking at the road or not, convert\n",
      "\n",
      "this information into a continuous estimate of (visual)\n",
      "\n",
      "distraction and finally use some rule, often a threshold, to\n",
      "\n",
      "determine if the estimated level of distraction should be\n",
      "\n",
      "considered distracted or attentive. The main limitation of\n",
      "\n",
      "these approaches is that they do not take the current traffic\n",
      "\n",
      "situation into account. This could be done by allowing the\n",
      "\n",
      "field relevant for driving to change dynamically over time.\n",
      "\n",
      "Future research is needed to (a) determine the optimal field\n",
      "\n",
      "relevant for driving for different traffic situations and traffic\n",
      "\n",
      "environments and (b) develop technology to be able to\n",
      "\n",
      "measure the current traffic situation and _ traffic\n",
      "\n",
      "environment.\n",
      "\n",
      "\n",
      "\n",
      "Only one of the available algorithms (percent road centre)\n",
      "\n",
      "was prepared in order to detect internal distraction.\n",
      "\n",
      "Suggested measures of internal distraction are based on the\n",
      "\n",
      "concentration of gazes towards the road centre area, which\n",
      "\n",
      "is higher when the driver is lost in thought. It has been\n",
      "\n",
      "suggested that other eye movements such as saccades and\n",
      "\n",
      "microsaccades could be indicative of workload or\n",
      "\n",
      "inattention. Future research is needed to (a) investigate eye\n",
      "\n",
      "movement physiology during driving, (b) develop remote\n",
      "\n",
      "eye tracking technology with higher accuracy so that these\n",
      "\n",
      "small and fast eye movements can be measured, and (c)\n",
      "\n",
      "develop algorithms that reliably and accurately detect\n",
      "\n",
      "different types of eye movements like fixations, saccades\n",
      "\n",
      "and smooth pursuit from the continuous data stream.\n",
      "\n",
      "\n",
      "\n",
      "Other distraction indicators such as lateral and longitudinal\n",
      "\n",
      "control parameters seem to be very task and situation\n",
      "\n",
      "dependent, and it is questionable whether they can be used\n",
      "\n",
      "in a general purpose driver distraction detection algorithm.\n",
      "\n",
      "Future research includes fusion of several data sources,\n",
      "\n",
      "including situational variables, so that the appropriate set of\n",
      "\n",
      "performance indicators is used at exactly the right place at\n",
      "\n",
      "the right time. Even though it might be impossible to\n",
      "\n",
      "replace eye movement related indicators completely withdriving related parameters, it would be very valuable to be\n",
      "\n",
      "able to fall back on this type of data when eye tracking is\n",
      "\n",
      "lost.\n",
      "\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "\n",
      "\n",
      "1. Gordon, C.P., Crash Studies of Driver Distraction, in\n",
      "\n",
      "Driver Distraction: Theory, effect and mitigation, M.A.\n",
      "\n",
      "Regan, J.D. Lee, and K.L. Young, Editors. CRC Press,\n",
      "\n",
      "Taylor & Francis Group, London (2009), 281-304.\n",
      "\n",
      "\n",
      "\n",
      "2. Lee, J.D., K.L. Young, and M.A. Regan, Defining\n",
      "\n",
      "Driver Distraction, in Driver Distraction: Theory, effect\n",
      "\n",
      "and mitigation, M.A. Regan, J.D. Lee, and K.L. Young,\n",
      "\n",
      "Editors. CRC Press, Taylor & Francis Group London\n",
      "\n",
      "(2009), 31-40.\n",
      "\n",
      "\n",
      "\n",
      "3. Recarte, M.A. and L.M. Nunes, Effects of verbal and\n",
      "\n",
      "spatial-imagery tasks on eye fixations while driving.\n",
      "\n",
      "Journal of Experimental Psychology: Applied, 6,1\n",
      "\n",
      "(2000), 31-43.\n",
      "\n",
      "\n",
      "\n",
      "4. Victor, T.W., J.L. Harbluk, and J. Engstrém, Sensitivity\n",
      "\n",
      "of eye-movement measures to in-vehicle task difficulty.\n",
      "\n",
      "Transportation Research Part F: Traffic Psychology and\n",
      "\n",
      "Behaviour, &, 2 (2005), 167-190.\n",
      "\n",
      "\n",
      "\n",
      "5. Engstrém, J., E. Johansson, and J. Ostlund, Effects of\n",
      "\n",
      "visual and cognitive load in real and_ simulated\n",
      "\n",
      "motorway driving. Transportation Research Part F:\n",
      "\n",
      "Traffic Psychology and Behaviour, 8, 2 (2005) 97-120.\n",
      "\n",
      "\n",
      "\n",
      "6. Green, P. and R. Shah, Task time and glance measures\n",
      "\n",
      "of the use of telematics: A tabular summary of the\n",
      "\n",
      "literature, in SAfety VEhicles using adaptive Interface\n",
      "\n",
      "Technology (2004), UMTRI: Ann Arbor.\n",
      "\n",
      "\n",
      "\n",
      "7. Zylstra, B., et al., Driving performance for dialing,\n",
      "\n",
      "radio tuning, and destination entry while driving\n",
      "\n",
      "straight roads. The University of Michigan\n",
      "\n",
      "Transportation Research Institute: Ann Arbor, MI\n",
      "\n",
      "(2003).\n",
      "\n",
      "\n",
      "\n",
      "8. Ostlund, J., et al., Driving performance assessment methods and metrics, in EU Deliverable, Adaptive\n",
      "\n",
      "Integrated Driver-Vehicle Interface Project (AIDE).\n",
      "\n",
      "(2005).\n",
      "\n",
      "\n",
      "\n",
      "9. Young, L.K., M.A. Regan, and J.D. Lee, Measuring the\n",
      "\n",
      "effects of driver distraction: Direct driving performance\n",
      "\n",
      "methods and measures, in Driver distraction: Theory,\n",
      "\n",
      "effects and mitigation, M.A. Regan, J.D. Lee, and K.L.\n",
      "\n",
      "Young, Editors. CRC PRess London (2009), 85-105.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "10. Victor, T.W., Keeping eye and mind on the road, in\n",
      "\n",
      "Dept of Psychology. Digital Comprehensive Summaries\n",
      "\n",
      "of Uppsala Dissertations from the Faculty of Social\n",
      "\n",
      "Sciences 9, Uppsala University: Uppsala (2005).\n",
      "\n",
      "\n",
      "\n",
      "11. Fletcher, L. and A. Zelinsky. Driver state monitoring to\n",
      "\n",
      "mitigate distraction. in International Conference on\n",
      "\n",
      "Driver Distraction. Sydney, Australia(2005).\n",
      "\n",
      "\n",
      "\n",
      "12. Engstrom, J. and S. Mardh, SafeTE Final Report(2007).\n",
      "\n",
      "\n",
      "\n",
      "13. Zhang, H., M. Smith, and R. Dufour, A Final Report of\n",
      "\n",
      "SAfety VEhicles using adaptive Interface Technology\n",
      "\n",
      "(Phase II: Task 7C): Visual Distraction (2008).\n",
      "\n",
      "\n",
      "\n",
      "14. Kircher, K. and C. Ahlstrom, Issues related to the driver\n",
      "\n",
      "distraction detection algorithm AttenD, in _ Ist\n",
      "\n",
      "International Conference on Driver Distraction and\n",
      "\n",
      "Inattention, Gothenburg, Sweden (2009).\n",
      "\n",
      "\n",
      "\n",
      "15. Kutila, M., et al. Driver Distraction Detection with a\n",
      "\n",
      "Camera Vision System. in Proceedings of the [EEE\n",
      "\n",
      "International Conference on Image Processing. San\n",
      "\n",
      "Antonio, Texas, US (2007).\n",
      "\n",
      "\n",
      "\n",
      "16.Pohl, J., W. Birk, and L. Westervall, A driverdistraction-based lane-keeping assistance system.\n",
      "\n",
      "Proceedings of the Institution of Mechanical Engineers\n",
      "\n",
      "Part I-Journal of Systems and Control Engineering, 221,\n",
      "\n",
      "14 (2007), 541-552.\n",
      "\n",
      "\n",
      "\n",
      "17.Donmez, B., L.N. Boyle, and J.D. Lee, Safety\n",
      "\n",
      "implications of providing real-time feedback to\n",
      "\n",
      "distracted drivers.Accident Analysis and Prevention, 39,\n",
      "\n",
      "3 (2007), 581-590.\n",
      "\n",
      "\n",
      "\n",
      "18. Lamble, D., M. Laakso, and H. Summala, Detection\n",
      "\n",
      "thresholds in car following situations and peripheral\n",
      "\n",
      "vision: implications for positioning of visually\n",
      "\n",
      "demanding in-car displays. Ergonomics, 42, 6, (1999),\n",
      "\n",
      "807-815.\n",
      "\n",
      "\n",
      "\n",
      "19. Liang, Y., Detecting Driver Distraction. University of\n",
      "\n",
      "Iowa (2009).\n",
      "\n",
      "\n",
      "\n",
      "20.Donmez, B., L.N. Boyle, and J.D. Lee, Designing\n",
      "\n",
      "feedback to mitigate distraction, in Driver distraction:\n",
      "\n",
      "Theory, effects and mitigation, M.A. Regan, J.D. Lee,\n",
      "\n",
      "and K.L. Young, Editors. CRC PRess London (2009),\n",
      "\n",
      "519-531.\n",
      "\n",
      "\n",
      "\n",
      "21. Kircher, K., A. Kircher, and F. Claezon, Distraction and\n",
      "\n",
      "drowsiness. A field study. VTI: Linképing (2009).Review of Real-time Visual Driver Distraction\n",
      "\n",
      "Detection Algorithms\n",
      "\n",
      "\n",
      "\n",
      "ChristerAhlIstrom\n",
      "\n",
      "christer.ahlstrom@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Katja Kircher\n",
      "\n",
      "katja.kircher@vti.se\n",
      "\n",
      "\n",
      "\n",
      "Swedish National Road and Transport Research Institute (VTI),\n",
      "\n",
      "581 95 Linkoping, Sweden\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "\n",
      "\n",
      "Many incidents and crashes can be attributed to driver\n",
      "\n",
      "distraction, and it is essential to learn how to\n",
      "\n",
      "detectdistraction in order to develop _ efficient\n",
      "\n",
      "countermeasures. A number of distraction detection\n",
      "\n",
      "algorithms have been developed over the years, and the\n",
      "\n",
      "objective of this paper is to summarize available approaches\n",
      "\n",
      "and to describe these algorithms in a unified framework.The\n",
      "\n",
      "review is limited to real-time algorithms that are intended to\n",
      "\n",
      "detect visual distraction.\n",
      "\n",
      "\n",
      "\n",
      "Author Keywords\n",
      "\n",
      "Driver distraction, eye tracking, inattention, detection\n",
      "\n",
      "algorithm.\n",
      "\n",
      "\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "\n",
      "\n",
      "Driver inattention and distraction are major contributors to\n",
      "\n",
      "road incidents and crashes [1, 2]. Even so, drivers continue\n",
      "\n",
      "to engage themselves in distracting tasks such as using their\n",
      "\n",
      "mobile phones or navigation systems, eating, grooming and\n",
      "\n",
      "tending to their children. Advanced driver assistance\n",
      "\n",
      "systems may change this pattern by alerting the driver when\n",
      "\n",
      "his or her attention diverts from the road.\n",
      "\n",
      "\n",
      "\n",
      "Driver distraction can be defined as “the diversion of\n",
      "\n",
      "attention away from activities critical for safe driving\n",
      "\n",
      "toward a competing activity” [2]. This is a very general\n",
      "\n",
      "definition where the diversion of attention can be visual,\n",
      "\n",
      "auditive, physical or cognitive and where the competing\n",
      "\n",
      "activity can be anything from mobile phone usage to getting\n",
      "\n",
      "lost in thought. New advances in remote eye tracking\n",
      "\n",
      "technology provide a means to counteract distracted driving\n",
      "\n",
      "in real-time. Eye movements can be used to gain access to\n",
      "\n",
      "several types of distraction. For example, studies have\n",
      "\n",
      "shown that eye movements are sensitive not only to visual\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Permission to make digital or hard copies of all or part of this work for\n",
      "\n",
      "personal or classroom use is granted without fee provided that copies are\n",
      "\n",
      "not made or distributed for profit or commercial advantage and that copies\n",
      "\n",
      "bear this notice and the full citation on the first page. Copyrights for\n",
      "\n",
      "components of this work owned by others than ACM must be honored.\n",
      "\n",
      "Abstracting with credit 1s permitted. To copy otherwise, to republish, to\n",
      "\n",
      "post of servers or redistribute to lists requires prior specific permission\n",
      "\n",
      "and/or a fee.\n",
      "\n",
      "\n",
      "\n",
      "MB’10 August 24-27, 2010, Eindhoven, Netherlands\n",
      "\n",
      "©ACM 2010 ISBN: 978-1-60558-926-8/10/08...$10.00\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "distraction but also to auditory secondary tasks [3-5].\n",
      "\n",
      "\n",
      "\n",
      "There are numerous performance indicators that are based\n",
      "\n",
      "on longitudinal and lateral vehicle control dynamicswhich\n",
      "\n",
      "correlate with visual as well as cognitive task demands] 68]. These include steering wheel reversal rate, average\n",
      "\n",
      "proportion of high frequency steering, brake reaction time,\n",
      "\n",
      "steering entropy, throttle hold, variability in lateral position,\n",
      "\n",
      "number of lane exceedences, time- and distance headway\n",
      "\n",
      "and time to collision. Even though many of these\n",
      "\n",
      "performance indicators seem to be promising secondary\n",
      "\n",
      "task identifiers, we restrict this survey to gaze based\n",
      "\n",
      "distraction detection algorithms.The objective of this paper\n",
      "\n",
      "is thus to summarize available real-time gaze based\n",
      "\n",
      "approaches for measuring visual driver distraction and to\n",
      "\n",
      "describe these algorithms in a unified framework. Since the\n",
      "\n",
      "focus of this review is on real-time assessment of visual\n",
      "\n",
      "distraction, many after-the-fact methods based on reaction\n",
      "\n",
      "times, secondary task performance and_ corrective\n",
      "\n",
      "manoeuvres are left out. A survey of the effects, in contrast\n",
      "\n",
      "to the prediction, of driver distraction can be found in\n",
      "\n",
      "Young et al. [9].\n",
      "\n",
      "\n",
      "\n",
      "PRINCIPLES OF DRIVER DISTRACTION DETECTION\n",
      "\n",
      "\n",
      "\n",
      "A schematic overview summarizing the structure of most\n",
      "\n",
      "driver distraction detection algorithms is illustrated in\n",
      "\n",
      "Figure 1. The basis for all algorithms is measures registered\n",
      "\n",
      "in real-time during driving. They can stem from the driver\n",
      "\n",
      "(driver behaviour), like eye movements or hand\n",
      "\n",
      "movements, or they can be logged from the vehicle (driving\n",
      "\n",
      "behaviour), like speed or lateral position. Furthermore,\n",
      "\n",
      "situational variables like time and position can be used\n",
      "\n",
      "(other data). Certain features of these data, like gaze\n",
      "\n",
      "direction, steering entropy or others are extracted and\n",
      "\n",
      "possibly fused in order to arrive at a continuous measure of\n",
      "\n",
      "the driver’s distraction level. This output is then used to\n",
      "\n",
      "classify the driver’s state of attention. For most algorithms\n",
      "\n",
      "these states are visually distracted vs. not visually\n",
      "\n",
      "distracted.\n",
      "\n",
      "\n",
      "\n",
      "Field Relevant for Driving\n",
      "\n",
      "\n",
      "\n",
      "Common for all eye or head movement based distraction\n",
      "\n",
      "algorithms is that they use off-road glances as the basic\n",
      "\n",
      "source of information. The idea is to define a field relevantdistraction detection algorithm\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "—\n",
      "\n",
      "5\n",
      "\n",
      "2\n",
      "\n",
      "— -——_—\n",
      "\n",
      "Oo &\n",
      "\n",
      ">\n",
      "\n",
      "‘= oO\n",
      "\n",
      "oa\n",
      "\n",
      "<\n",
      "\n",
      "2\n",
      "\n",
      "wn = 6\n",
      "\n",
      "wo > gS c¢\n",
      "\n",
      "5 op -S ® 2\n",
      "\n",
      "a —— ___ dg o __ a\n",
      "\n",
      "© se wo >\n",
      "\n",
      "wy = Ow = —\n",
      "\n",
      "£ >o 2 a\n",
      "\n",
      "&\n",
      "\n",
      "@\n",
      "\n",
      "—\n",
      "\n",
      "ros)\n",
      "\n",
      "~Y\n",
      "\n",
      "so\n",
      "\n",
      "ra —<$—$————$ >\n",
      "\n",
      "@\n",
      "\n",
      "<=\n",
      "\n",
      "~\n",
      "\n",
      "oO\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "classification\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "warning algorithm\n",
      "\n",
      "\n",
      "\n",
      "not\n",
      "\n",
      "nn > distracted\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "no warning\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "-—- - Se FH —\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "| I\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Bo warning\n",
      "\n",
      "Ww\n",
      "\n",
      "\n",
      "\n",
      "——- -— = — > i\n",
      "\n",
      "oa)\n",
      "\n",
      "WN\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 1. A schematic overview of the common features of most driver distraction detection algorithms and mitigation systems.\n",
      "\n",
      "\n",
      "\n",
      "for driving, which is basically the area where the driver is\n",
      "\n",
      "looking when he or she is driving. If a world model is not\n",
      "\n",
      "available, the field relevant for driving can, for example, be\n",
      "\n",
      "defined as a circle [10-12] or a rectangle [13], see Figure 2.\n",
      "\n",
      "It is also possible to select different shapes. In Kircher et al.\n",
      "\n",
      "[14], a circle where the lower part was removed was used\n",
      "\n",
      "so that the dashboard would not be included in the field\n",
      "\n",
      "relevant for driving. Since there is no information about\n",
      "\n",
      "where the driver is looking in the real world, the selected\n",
      "\n",
      "field relevant for driving needs to be positioned in the real\n",
      "\n",
      "world based on statistics of where the driver has been\n",
      "\n",
      "looking. This is often done by centering the selected shape\n",
      "\n",
      "around the largest peak in the distribution of recent gazes.\n",
      "\n",
      "When enough gaze data has been acquired, it is also\n",
      "\n",
      "possible to define more than one zone based on the\n",
      "\n",
      "distribution of the data. For example, Kutila et al. [15] uses\n",
      "\n",
      "four zones (road ahead, windscreen and left/right exterior\n",
      "\n",
      "mirror.\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Figure 2. Examples of three different approaches to select\n",
      "\n",
      "the field relevant from driving. The leftmost plot show a\n",
      "\n",
      "circular area, the middle plot shows a rectangular area and\n",
      "\n",
      "the rightmost plot shows an area that is defined based on the\n",
      "\n",
      "cockpit of the car.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "If the eye tracking systems allows a world model to be\n",
      "\n",
      "used, the field relevant for driving can be defined based on\n",
      "\n",
      "different zones related to the interior of the car. This\n",
      "\n",
      "approach is used by Pohl et al. [16] and Kircher et al. [14],\n",
      "\n",
      "see Figure 2. In the latter of these two, the field relevant for\n",
      "\n",
      "driving is defined as the intersection between a viewing\n",
      "\n",
      "cone of 90 degrees and the vehicle’s windows. This means\n",
      "\n",
      "that the circular field relevant for driving concept is\n",
      "\n",
      "expanded with information about the design of the car.\n",
      "\n",
      "\n",
      "\n",
      "Distraction Estimation\n",
      "\n",
      "\n",
      "\n",
      "Glances away from the road ahead are usually defined as\n",
      "\n",
      "glances residing outside the field relevant for driving. The\n",
      "\n",
      "duration of these glances away from the road ahead is the\n",
      "\n",
      "basic source of information that all visual distraction\n",
      "\n",
      "detection algorithms to date are based upon. If the driver is\n",
      "\n",
      "looking away from the road too often or for too long, the\n",
      "\n",
      "driver is considered distracted.\n",
      "\n",
      "\n",
      "\n",
      "The mappings that transform glances away from the road to\n",
      "\n",
      "a continuous distraction estimate are often very similar. For\n",
      "\n",
      "example, Zhang et al. [13] used the average duration of\n",
      "\n",
      "glances away from the road in a 4.3-second wide sliding\n",
      "\n",
      "window, Donmez et al. [17] used a weighted sum of the\n",
      "\n",
      "current glance and the average glance duration in a 3second sliding window and Victor [10] used the percentage\n",
      "\n",
      "of on-road gaze data points in a 60-second sliding window.\n",
      "\n",
      "A slightly different approach is to use a buffer [14] or a\n",
      "\n",
      "counter [11] that changes its value when the driver looks\n",
      "\n",
      "away. Here the counter/buffer reaches amaximum/minimum value when the driver is judged to be\n",
      "\n",
      "too distracted.\n",
      "\n",
      "\n",
      "\n",
      "So far, there has been a direct link from the FRD via the\n",
      "\n",
      "glance duration to the estimated distraction level in the\n",
      "\n",
      "sense that all gazes have the same weight, regardless of\n",
      "\n",
      "where the gaze is directed. However, it is possible to make\n",
      "\n",
      "this link fuzzier by changing the weight as a function of\n",
      "\n",
      "where the gaze is directed. One idea is thus to penalize\n",
      "\n",
      "glances that are far away from the road centre. In the\n",
      "\n",
      "SafeTE project [12], this was done by the so-called\n",
      "\n",
      "eccentricity function E(a) = 6.5758—-1/(0.001*a + 0.152).\n",
      "\n",
      "This is basically a weighting function that favours glances\n",
      "\n",
      "close to the road centre while penalizing glances with a\n",
      "\n",
      "large gaze direction angle. The equation is based on a study\n",
      "\n",
      "by Lamble et al. [18] and is related to visual behaviour and\n",
      "\n",
      "brake response when a lead vehicle suddenly starts to\n",
      "\n",
      "decelerate. In cases where a world model is available, it is\n",
      "\n",
      "possible to use different weights on different objects [14,\n",
      "\n",
      "16]. For example, the rear view mirrors and_ the\n",
      "\n",
      "speedometer could have a higher weight as compared to the\n",
      "\n",
      "field relevant for driving but lower than the middle console\n",
      "\n",
      "or the glove compartment. Higher weights in this context\n",
      "\n",
      "mean that the distraction estimate will increase faster while\n",
      "\n",
      "lower weights have the opposite effect.Other combinations\n",
      "\n",
      "of the distraction estimation functions mentioned above, 1.e.\n",
      "\n",
      "glance duration, glance history and eccentricity,has also\n",
      "\n",
      "been suggested [19].\n",
      "\n",
      "\n",
      "\n",
      "Distraction Decision\n",
      "\n",
      "\n",
      "\n",
      "The continuous distraction estimate needs to be mapped to a\n",
      "\n",
      "decision whether the driver is distracted or not. Basically,\n",
      "\n",
      "the driver enters the distracted state when a threshold is\n",
      "\n",
      "reached and returns to the attentive state when some criteria\n",
      "\n",
      "are fulfilled. The main difference between different\n",
      "\n",
      "approaches is how to leave the distracted state. One\n",
      "\n",
      "approach is to require that the driver is looking forward for\n",
      "\n",
      "some minimum time before he or she is considered to be\n",
      "\n",
      "attentive [14, 16, 17]. The other approach is that it is\n",
      "\n",
      "enough for the driver to look back at the road to be\n",
      "\n",
      "considered fully attentive [11].\n",
      "\n",
      "\n",
      "\n",
      "Inhibition Criteria\n",
      "\n",
      "\n",
      "\n",
      "A distraction detection algorithm determines whether a\n",
      "\n",
      "driver is distracted or not, but when and in which way the\n",
      "\n",
      "driver will be warned for distraction is determined by a\n",
      "\n",
      "warning strategy. Information about different warning\n",
      "\n",
      "strategies is out of the scope of this review. More\n",
      "\n",
      "information can be found in, for example, Donmez et al.\n",
      "\n",
      "[20]. However, there are situations when it is not suitable to\n",
      "\n",
      "give distraction warnings. For instance, if the driver is\n",
      "\n",
      "braking hard he or she is probably aware of the situation\n",
      "\n",
      "and should not be disturbed by a warning. For this reason,\n",
      "\n",
      "certain criteria can be set up to inhibit warnings. Common\n",
      "\n",
      "criteria include [21]:\n",
      "\n",
      "\n",
      "\n",
      "e Speed: Below 50 km/h gaze behaviour is not very\n",
      "\n",
      "uniform. The gaze is often outside the FRD without the\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "driver being distracted.\n",
      "\n",
      "\n",
      "\n",
      "e Direction indicators: Changing lanes and turning can\n",
      "\n",
      "include planned glances outside the FRD.\n",
      "\n",
      "\n",
      "\n",
      "e Reverse gear: Reverse engaged means that the driver\n",
      "\n",
      "should look over the shoulder.\n",
      "\n",
      "\n",
      "\n",
      "e Brake pedal: No warning should be given while driver is\n",
      "\n",
      "braking, in order not to interfere with critical driving\n",
      "\n",
      "manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Steering wheel angle: No warning should be given while\n",
      "\n",
      "the driver is engaged in substantial changes of direction,\n",
      "\n",
      "in order not to interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "e Lateral acceleration: No warning should be given when\n",
      "\n",
      "the vehicle makes strong movements, in order not to\n",
      "\n",
      "interfere with critical driving manoeuvres.\n",
      "\n",
      "\n",
      "\n",
      "IMPROVEMENTS AND FUTURE RESEARCH\n",
      "\n",
      "\n",
      "\n",
      "Available algorithms for eye tracking based driver\n",
      "\n",
      "distraction detection attempt to detect visual distraction.\n",
      "\n",
      "All algorithms can be fitted in a common framework;\n",
      "\n",
      "determine if the driver is looking at the road or not, convert\n",
      "\n",
      "this information into a continuous estimate of (visual)\n",
      "\n",
      "distraction and finally use some rule, often a threshold, to\n",
      "\n",
      "determine if the estimated level of distraction should be\n",
      "\n",
      "considered distracted or attentive. The main limitation of\n",
      "\n",
      "these approaches is that they do not take the current traffic\n",
      "\n",
      "situation into account. This could be done by allowing the\n",
      "\n",
      "field relevant for driving to change dynamically over time.\n",
      "\n",
      "Future research is needed to (a) determine the optimal field\n",
      "\n",
      "relevant for driving for different traffic situations and traffic\n",
      "\n",
      "environments and (b) develop technology to be able to\n",
      "\n",
      "measure the current traffic situation and _ traffic\n",
      "\n",
      "environment.\n",
      "\n",
      "\n",
      "\n",
      "Only one of the available algorithms (percent road centre)\n",
      "\n",
      "was prepared in order to detect internal distraction.\n",
      "\n",
      "Suggested measures of internal distraction are based on the\n",
      "\n",
      "concentration of gazes towards the road centre area, which\n",
      "\n",
      "is higher when the driver is lost in thought. It has been\n",
      "\n",
      "suggested that other eye movements such as saccades and\n",
      "\n",
      "microsaccades could be indicative of workload or\n",
      "\n",
      "inattention. Future research is needed to (a) investigate eye\n",
      "\n",
      "movement physiology during driving, (b) develop remote\n",
      "\n",
      "eye tracking technology with higher accuracy so that these\n",
      "\n",
      "small and fast eye movements can be measured, and (c)\n",
      "\n",
      "develop algorithms that reliably and accurately detect\n",
      "\n",
      "different types of eye movements like fixations, saccades\n",
      "\n",
      "and smooth pursuit from the continuous data stream.\n",
      "\n",
      "\n",
      "\n",
      "Other distraction indicators such as lateral and longitudinal\n",
      "\n",
      "control parameters seem to be very task and situation\n",
      "\n",
      "dependent, and it is questionable whether they can be used\n",
      "\n",
      "in a general purpose driver distraction detection algorithm.\n",
      "\n",
      "Future research includes fusion of several data sources,\n",
      "\n",
      "including situational variables, so that the appropriate set of\n",
      "\n",
      "performance indicators is used at exactly the right place at\n",
      "\n",
      "the right time. Even though it might be impossible to\n",
      "\n",
      "replace eye movement related indicators completely withdriving related parameters, it would be very valuable to be\n",
      "\n",
      "able to fall back on this type of data when eye tracking is\n",
      "\n",
      "lost.\n",
      "\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "\n",
      "\n",
      "1. Gordon, C.P., Crash Studies of Driver Distraction, in\n",
      "\n",
      "Driver Distraction: Theory, effect and mitigation, M.A.\n",
      "\n",
      "Regan, J.D. Lee, and K.L. Young, Editors. CRC Press,\n",
      "\n",
      "Taylor & Francis Group, London (2009), 281-304.\n",
      "\n",
      "\n",
      "\n",
      "2. Lee, J.D., K.L. Young, and M.A. Regan, Defining\n",
      "\n",
      "Driver Distraction, in Driver Distraction: Theory, effect\n",
      "\n",
      "and mitigation, M.A. Regan, J.D. Lee, and K.L. Young,\n",
      "\n",
      "Editors. CRC Press, Taylor & Francis Group London\n",
      "\n",
      "(2009), 31-40.\n",
      "\n",
      "\n",
      "\n",
      "3. Recarte, M.A. and L.M. Nunes, Effects of verbal and\n",
      "\n",
      "spatial-imagery tasks on eye fixations while driving.\n",
      "\n",
      "Journal of Experimental Psychology: Applied, 6,1\n",
      "\n",
      "(2000), 31-43.\n",
      "\n",
      "\n",
      "\n",
      "4. Victor, T.W., J.L. Harbluk, and J. Engstrém, Sensitivity\n",
      "\n",
      "of eye-movement measures to in-vehicle task difficulty.\n",
      "\n",
      "Transportation Research Part F: Traffic Psychology and\n",
      "\n",
      "Behaviour, &, 2 (2005), 167-190.\n",
      "\n",
      "\n",
      "\n",
      "5. Engstrém, J., E. Johansson, and J. Ostlund, Effects of\n",
      "\n",
      "visual and cognitive load in real and_ simulated\n",
      "\n",
      "motorway driving. Transportation Research Part F:\n",
      "\n",
      "Traffic Psychology and Behaviour, 8, 2 (2005) 97-120.\n",
      "\n",
      "\n",
      "\n",
      "6. Green, P. and R. Shah, Task time and glance measures\n",
      "\n",
      "of the use of telematics: A tabular summary of the\n",
      "\n",
      "literature, in SAfety VEhicles using adaptive Interface\n",
      "\n",
      "Technology (2004), UMTRI: Ann Arbor.\n",
      "\n",
      "\n",
      "\n",
      "7. Zylstra, B., et al., Driving performance for dialing,\n",
      "\n",
      "radio tuning, and destination entry while driving\n",
      "\n",
      "straight roads. The University of Michigan\n",
      "\n",
      "Transportation Research Institute: Ann Arbor, MI\n",
      "\n",
      "(2003).\n",
      "\n",
      "\n",
      "\n",
      "8. Ostlund, J., et al., Driving performance assessment methods and metrics, in EU Deliverable, Adaptive\n",
      "\n",
      "Integrated Driver-Vehicle Interface Project (AIDE).\n",
      "\n",
      "(2005).\n",
      "\n",
      "\n",
      "\n",
      "9. Young, L.K., M.A. Regan, and J.D. Lee, Measuring the\n",
      "\n",
      "effects of driver distraction: Direct driving performance\n",
      "\n",
      "methods and measures, in Driver distraction: Theory,\n",
      "\n",
      "effects and mitigation, M.A. Regan, J.D. Lee, and K.L.\n",
      "\n",
      "Young, Editors. CRC PRess London (2009), 85-105.\n",
      "\n",
      "\n",
      "\n",
      "RIGHTS LIN K4p\n",
      "\n",
      "\n",
      "\n",
      "10. Victor, T.W., Keeping eye and mind on the road, in\n",
      "\n",
      "Dept of Psychology. Digital Comprehensive Summaries\n",
      "\n",
      "of Uppsala Dissertations from the Faculty of Social\n",
      "\n",
      "Sciences 9, Uppsala University: Uppsala (2005).\n",
      "\n",
      "\n",
      "\n",
      "11. Fletcher, L. and A. Zelinsky. Driver state monitoring to\n",
      "\n",
      "mitigate distraction. in International Conference on\n",
      "\n",
      "Driver Distraction. Sydney, Australia(2005).\n",
      "\n",
      "\n",
      "\n",
      "12. Engstrom, J. and S. Mardh, SafeTE Final Report(2007).\n",
      "\n",
      "\n",
      "\n",
      "13. Zhang, H., M. Smith, and R. Dufour, A Final Report of\n",
      "\n",
      "SAfety VEhicles using adaptive Interface Technology\n",
      "\n",
      "(Phase II: Task 7C): Visual Distraction (2008).\n",
      "\n",
      "\n",
      "\n",
      "14. Kircher, K. and C. Ahlstrom, Issues related to the driver\n",
      "\n",
      "distraction detection algorithm AttenD, in _ Ist\n",
      "\n",
      "International Conference on Driver Distraction and\n",
      "\n",
      "Inattention, Gothenburg, Sweden (2009).\n",
      "\n",
      "\n",
      "\n",
      "15. Kutila, M., et al. Driver Distraction Detection with a\n",
      "\n",
      "Camera Vision System. in Proceedings of the [EEE\n",
      "\n",
      "International Conference on Image Processing. San\n",
      "\n",
      "Antonio, Texas, US (2007).\n",
      "\n",
      "\n",
      "\n",
      "16.Pohl, J., W. Birk, and L. Westervall, A driverdistraction-based lane-keeping assistance system.\n",
      "\n",
      "Proceedings of the Institution of Mechanical Engineers\n",
      "\n",
      "Part I-Journal of Systems and Control Engineering, 221,\n",
      "\n",
      "14 (2007), 541-552.\n",
      "\n",
      "\n",
      "\n",
      "17.Donmez, B., L.N. Boyle, and J.D. Lee, Safety\n",
      "\n",
      "implications of providing real-time feedback to\n",
      "\n",
      "distracted drivers.Accident Analysis and Prevention, 39,\n",
      "\n",
      "3 (2007), 581-590.\n",
      "\n",
      "\n",
      "\n",
      "18. Lamble, D., M. Laakso, and H. Summala, Detection\n",
      "\n",
      "thresholds in car following situations and peripheral\n",
      "\n",
      "vision: implications for positioning of visually\n",
      "\n",
      "demanding in-car displays. Ergonomics, 42, 6, (1999),\n",
      "\n",
      "807-815.\n",
      "\n",
      "\n",
      "\n",
      "19. Liang, Y., Detecting Driver Distraction. University of\n",
      "\n",
      "Iowa (2009).\n",
      "\n",
      "\n",
      "\n",
      "20.Donmez, B., L.N. Boyle, and J.D. Lee, Designing\n",
      "\n",
      "feedback to mitigate distraction, in Driver distraction:\n",
      "\n",
      "Theory, effects and mitigation, M.A. Regan, J.D. Lee,\n",
      "\n",
      "and K.L. Young, Editors. CRC PRess London (2009),\n",
      "\n",
      "519-531.\n",
      "\n",
      "\n",
      "\n",
      "21. Kircher, K., A. Kircher, and F. Claezon, Distraction and\n",
      "\n",
      "drowsiness. A field study. VTI: Linképing (2009).\n"
     ]
    }
   ],
   "source": [
    "with open(fileName,'r') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
